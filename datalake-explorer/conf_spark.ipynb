{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class SettingsSpark:\n",
    "    \n",
    "    def __init__(self, instance=None, cores=None, driver_memory=None, executor_memory=None):\n",
    "        self.instance = instance or \"2\"\n",
    "        self.cores = cores or \"1\"\n",
    "        self.driver_memory = driver_memory or \"512m\"\n",
    "        self.executor_memory = executor_memory or \"758m\"\n",
    "        self.__init_conf__()\n",
    "        self.sc = self.builder_conf().sparkContext\n",
    "        self.log_level(\"OFF\")\n",
    "        \n",
    "    \n",
    "    def __init_conf__(self):\n",
    "        conf = ( \n",
    "            SparkConf()\n",
    "            .setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")  \n",
    "            .setAppName(\"spark\") \n",
    "            .set(\"spark.kubernetes.container.image\", \"magaiwer/pyspark-notebook:3.2.1\") \n",
    "            .set(\"spark.kubernetes.namespace\", \"spark\") \n",
    "            .set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\") \n",
    "            .set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\") \n",
    "            .set(\"spark.executor.instances\", self.instance) \n",
    "            .set(\"spark.executor.cores\", self.cores) \n",
    "            .set(\"spark.driver.memory\", self.driver_memory) \n",
    "            .set(\"spark.executor.memory\", self.executor_memory) \n",
    "            .set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-sa\") \n",
    "            .set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark-sa\") \n",
    "            .set(\"spark.driver.port\", \"29413\") \n",
    "            .set(\"spark.driver.host\", \"jupyter-labs.spark.svc.cluster.local\") \n",
    "            .set(\"spark.driver.bindAddress\", \"0.0.0.0\") \n",
    "            .set(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "        )\n",
    "        return conf\n",
    "    \n",
    "    def builder_conf(self):\n",
    "        return SparkSession.builder.config(conf=self.__init_conf__()).getOrCreate()\n",
    "    \n",
    "    def init_minio(self, acess_key=None, secret_key=None, endpoint=None):\n",
    "        self.acess_key = acess_key or \"admin\"\n",
    "        self.secret_key = secret_key or \"e7bc4dc8-3abf-4187-bcc8-5a4bc8dc32e1\"\n",
    "        self.endpoint = endpoint or \"https://api.minio.magaiver.dev\"\n",
    "    \n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", self.acess_key)\n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", self.secret_key)\n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", self.endpoint)\n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.fast.upload\", \"true\")\n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"2000\")\n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"2000\")\n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"2\")\n",
    "        self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\",\"false\")\n",
    "    \n",
    "    def stop(self):\n",
    "        self.sc.stop()\n",
    "    \n",
    "    def log_level(self, level=None):\n",
    "        lev = level or \"OFF\"\n",
    "        self.sc.setLogLevel(lev)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30395/3857523924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSettingsSpark\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T00:49:01.543617Z",
     "iopub.status.busy": "2021-11-14T00:49:01.542326Z",
     "iopub.status.idle": "2021-11-14T00:49:01.885930Z",
     "shell.execute_reply": "2021-11-14T00:49:01.884210Z",
     "shell.execute_reply.started": "2021-11-14T00:49:01.543478Z"
    },
    "tags": []
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "93a2936ef589dd946f11e51be3b2fffcb0e2acbd14ba7e47ba0d8aa70c513569"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}